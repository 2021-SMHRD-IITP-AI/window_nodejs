{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "pip install requests"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (2.25.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from requests) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from requests) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from requests) (4.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "pip install beautifulsoup4"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "pip install pyperclip"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: pyperclip in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (1.8.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "pip install selenium"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (3.141.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: urllib3 in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from selenium) (1.26.4)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "pip install openpyxl"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (3.0.7)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\smhrd\\anaconda3\\lib\\site-packages (from openpyxl) (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "import requests\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "import json\r\n",
    "from collections import OrderedDict\r\n",
    "import pymysql\r\n",
    "import pymysql.cursors\r\n",
    "\r\n",
    "conn = pymysql.connect(host='localhost',user='root',password=1234, db = 'nodejs_db',charset='utf8mb4')\r\n",
    "\r\n",
    "article_data = OrderedDict()\r\n",
    "\r\n",
    "webpage = requests.get(\"https://academic.naver.com/article.naver?doc_id=553446304\")\r\n",
    "soup = BeautifulSoup(webpage.content, \"html.parser\")\r\n",
    "\r\n",
    "# 초기 데이터 입력\r\n",
    "article_data[\"articleTitle\"] = soup.find('h4',{\"id\":\"articleData\"}).text\r\n",
    "article_data[\"articleId\"] = 553446304\r\n",
    "article_data[\"link\"] = 'https://academic.naver.com/article.naver?doc_id=553446304'\r\n",
    "article_data[\"authorName\"] = soup.find('dd',{\"class\":\"ui_listdetail_item_info\"}).text\r\n",
    "article_data[\"citatedCount\"] = soup.select('div.type2 dt.ui_listdetail_item_tit+dd.ui_listdetail_item_info')[3].text # 형변환 불가. 확인필요\r\n",
    "article_data[\"category\"] = soup.find('dd',{\"class\":\"category_item\"}).text.replace('\\n',\"\").replace('\\t',\"\").replace('\\r',\"\")\r\n",
    "article_data[\"regDate\"] = int(soup.select('dd.ui_listdetail_item_info>span.ui_listdetail_box_txt+span.ui_listdetail_box_txt')[0].text[0:4])\r\n",
    "article_data[\"keyword\"] = soup.select('dd.category_item+dt.ui_listdetail_item_tit+dd.ui_listdetail_item_info')[0].text\r\n",
    "article_data[\"abstract\"] = soup.find('p',{\"class\":\"ui_enddetail_txt\"}).text.strip()\r\n",
    "\r\n",
    "# articleTitle = soup.find('h4',{\"id\":\"articleData\"}).text\r\n",
    "# articleId = 553446304\r\n",
    "# link = 'https://academic.naver.com/article.naver?doc_id=553446304'\r\n",
    "# authorName = soup.find('dd',{\"class\":\"ui_listdetail_item_info\"}).text\r\n",
    "# citatedCount = soup.select('div.type2 dt.ui_listdetail_item_tit+dd.ui_listdetail_item_info')[3].text # 형변환 불가. 확인필요\r\n",
    "# category = soup.find('dd',{\"class\":\"category_item\"}).text.replace('\\n',\"\").replace('\\t',\"\").replace('\\r',\"\")\r\n",
    "# regDate = int(soup.select('dd.ui_listdetail_item_info>span.ui_listdetail_box_txt+span.ui_listdetail_box_txt')[0].text[0:4])\r\n",
    "# keyword = soup.select('dd.category_item+dt.ui_listdetail_item_tit+dd.ui_listdetail_item_info')[0].text\r\n",
    "# abstract = soup.find('p',{\"class\":\"ui_enddetail_txt\"}).text.strip()\r\n",
    "\r\n",
    "citation_data = OrderedDict()\r\n",
    "\r\n",
    "# citation_data[\"aId\"] = 553446304\r\n",
    "citation_data[\"aId\"] = article_data[\"articleId\"]\r\n",
    "count = 0\r\n",
    "\r\n",
    "for i in soup.find_all(attrs={\"class\":\"ui_listing_subtit\"}) and abs_link:\r\n",
    "\r\n",
    "    href = i.attrs['href'] # a태그의 href 속성에서 추출\r\n",
    "    \r\n",
    "    # 참고문헌 table citation 컬럼 데이터를 json citation_data에 입력\r\n",
    "    cId = \"cId\"\r\n",
    "    cNumber = str(count+1)\r\n",
    "    cur_cId = cId+cNumber\r\n",
    "    citation_data[cur_cId] = int(href[22:31]) \r\n",
    "    count+=1\r\n",
    "\r\n",
    "    f.write(href[22:31]+'\\n')\r\n",
    "    f.write(i.text+'\\n') #참고문헌\r\n",
    "\r\n",
    "# 데이터 저장\r\n",
    "\r\n",
    "with open('article_data','w',encoding=\"utf-8\") as make_file:\r\n",
    "    json.dump(article_data,make_file,ensure_ascii=False, indent=\"\\t\")\r\n",
    "\r\n",
    "with open('citation_data','w',encoding=\"utf-8\") as make_file:\r\n",
    "    json.dump(citation_data,make_file,ensure_ascii=False, indent=\"\\t\")\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "abs_link=soup.select('ul.ui_listing_item a[href^=\"/article.naver?doc_id=\"]')\r\n",
    "\r\n",
    "cnt = 1\r\n",
    "for a in abs_link:\r\n",
    "    href = a.attrs['href'] # a태그의 href 속성에서 추출\r\n",
    "    # f.write('\\n')\r\n",
    "    # f.write(href[22:31]+'\\n')\r\n",
    "    \r\n",
    "    webpage = requests.get(\"https://academic.naver.com/article.naver?doc_id=\" + str(href[22:31]))\r\n",
    "    soup = BeautifulSoup(webpage.content, \"html.parser\")\r\n",
    "    globals()['article_data_{}'.format(cnt)] = OrderedDict()\r\n",
    "    try:\r\n",
    "        # f.write(soup.find('h4',{\"id\":\"articleData\"}).text+'\\n') #제목\r\n",
    "        # f.write(soup.find('dd',{\"class\":\"ui_listdetail_item_info\"}).text+'\\n') #저자\r\n",
    "        # f.write(soup.select('dd.ui_listdetail_item_info>span.ui_listdetail_box_txt+span.ui_listdetail_box_txt')[0].text[0:4]+'\\n') #발행날짜\r\n",
    "        # f.write(soup.select('div.type2 dt.ui_listdetail_item_tit+dd.ui_listdetail_item_info')[3].text+'\\n') #피인용\r\n",
    "        # f.write(soup.find('dd',{\"class\":\"category_item\"}).text.strip()+'\\n') #소분류\r\n",
    "        # f.write(soup.select('dd.category_item+dt.ui_listdetail_item_tit+dd.ui_listdetail_item_info')[0].text+'\\n') #키워드\r\n",
    "\r\n",
    "        globals()['article_data_{}'.format(cnt)][\"articleTitle\"] = soup.find('h4',{\"id\":\"articleData\"}).text\r\n",
    "        globals()['article_data_{}'.format(cnt)][\"articleId\"] = 553446304\r\n",
    "        globals()['article_data_{}'.format(cnt)][\"link\"] = 'https://academic.naver.com/article.naver?doc_id=553446304'\r\n",
    "        globals()['article_data_{}'.format(cnt)][\"authorName\"] = soup.find('dd',{\"class\":\"ui_listdetail_item_info\"}).text\r\n",
    "        globals()['article_data_{}'.format(cnt)][\"citatedCount\"] = soup.select('div.type2 dt.ui_listdetail_item_tit+dd.ui_listdetail_item_info')[3].text # 형변환 불가. 확인필요\r\n",
    "        globals()['article_data_{}'.format(cnt)][\"category\"] = soup.find('dd',{\"class\":\"category_item\"}).text.replace('\\n',\"\").replace('\\t',\"\").replace('\\r',\"\")\r\n",
    "        globals()['article_data_{}'.format(cnt)][\"regDate\"] = int(soup.select('dd.ui_listdetail_item_info>span.ui_listdetail_box_txt+span.ui_listdetail_box_txt')[0].text[0:4])\r\n",
    "        globals()['article_data_{}'.format(cnt)][\"keyword\"] = soup.select('dd.category_item+dt.ui_listdetail_item_tit+dd.ui_listdetail_item_info')[0].text\r\n",
    "    \r\n",
    "    except Exception as e :\r\n",
    "        continue\r\n",
    "\r\n",
    "    try :\r\n",
    "        # f.write(soup.find('p',{\"class\":\"ui_enddetail_txt\"}).text.strip()+'\\n') #초록\r\n",
    "        globals()['article_data_{}'.format(cnt)][\"abstract\"] = soup.find('p',{\"class\":\"ui_enddetail_txt\"}).text.strip()\r\n",
    "\r\n",
    "    except Exception as e:\r\n",
    "        continue\r\n",
    "\r\n",
    "    finally:\r\n",
    "        \r\n",
    "        globals()['citation_data _{}'.format(cnt)] = OrderedDict()\r\n",
    "        # citation_data[\"aId\"] = 553446304\r\n",
    "        globals()['citation_data _{}'.format(cnt)][\"aId\"] = article_data_cnt[\"articleId\"]\r\n",
    "        count = 0\r\n",
    "        for i in soup.find_all(attrs={\"class\":\"ui_listing_subtit\"}) and abs_link:\r\n",
    "            try :\r\n",
    "                # href = i.attrs['href'] # a태그의 href 속성에서 추출\r\n",
    "                # f.write(href[22:31]+'\\n')\r\n",
    "                # f.write(i.text+'\\n') #참고문헌\r\n",
    "                cId = \"cId\"\r\n",
    "                cNumber = str(count+1)\r\n",
    "                cur_cId = cId+cNumber\r\n",
    "                citation_data[cur_cId] = int(href[22:31]) \r\n",
    "                count+=1\r\n",
    "\r\n",
    "                with open(globals()['citation_data _{}'.format(cnt)],'w',encoding=\"utf-8\") as make_file:\r\n",
    "                    json.dump(globals()['citation_data _{}'.format(cnt)],make_file,ensure_ascii=False, indent=\"\\t\")\r\n",
    "            except Exception as e:\r\n",
    "                # f.write(i.text+'\\n')\r\n",
    "\r\n",
    "    \r\n",
    "\r\n",
    "    \r\n",
    "            \r\n",
    "    # abs_link=soup.select('ul.ui_listing_item a[href^=\"/article.naver?doc_id=\"]')\r\n",
    "#     for a in abs_link:\r\n",
    "#         href = a.attrs['href'] # a태그의 href 속성에서 추출\r\n",
    "#         f.write('\\n')\r\n",
    "#         f.write(href[22:31]+'\\n')\r\n",
    "\r\n",
    "#         webpage = requests.get(\"https://academic.naver.com/article.naver?doc_id=\" + str(href[22:31]))\r\n",
    "#         soup = BeautifulSoup(webpage.content, \"html.parser\")\r\n",
    "        \r\n",
    "#         try:\r\n",
    "#             f.write(soup.find('h4',{\"id\":\"articleData\"}).text+'\\n') #제목\r\n",
    "#             f.write(soup.find('dd',{\"class\":\"ui_listdetail_item_info\"}).text+'\\n') #저자\r\n",
    "#             f.write(soup.select('dd.ui_listdetail_item_info>span.ui_listdetail_box_txt+span.ui_listdetail_box_txt')[0].text[0:4]+'\\n') #발행날짜\r\n",
    "#             f.write(soup.select('div.type2 dt.ui_listdetail_item_tit+dd.ui_listdetail_item_info')[3].text+'\\n') #피인용\r\n",
    "#             f.write(soup.find('dd',{\"class\":\"category_item\"}).text.strip()+'\\n') #소분류\r\n",
    "#             f.write(soup.select('dd.category_item+dt.ui_listdetail_item_tit+dd.ui_listdetail_item_info')[0].text+'\\n') #키워드\r\n",
    "#         except Exception as e :\r\n",
    "#             continue\r\n",
    "#         try :\r\n",
    "#             f.write(soup.find('p',{\"class\":\"ui_enddetail_txt\"}).text.strip()+'\\n') #초록\r\n",
    "#         except Exception as e:\r\n",
    "#             continue\r\n",
    "#         finally:\r\n",
    "#             for i in soup.find_all(attrs={\"class\":\"ui_listing_subtit\"}):\r\n",
    "#                 try :\r\n",
    "#                     href = i.attrs['href'] # a태그의 href 속성에서 추출\r\n",
    "#                     f.write(href[22:31]+'\\n')\r\n",
    "#                     f.write(i.text+'\\n') #참고문헌\r\n",
    "#                 except Exception as e:\r\n",
    "#                     f.write(i.text+'\\n')\r\n",
    "\r\n",
    "                \r\n",
    "#         abs_link=soup.select('ul.ui_listing_item a[href^=\"/article.naver?doc_id=\"]')\r\n",
    "#         for a in abs_link:\r\n",
    "#             href = a.attrs['href'] # a태그의 href 속성에서 추출\r\n",
    "#             f.write('\\n')\r\n",
    "#             f.write(href[22:31]+'\\n')\r\n",
    "\r\n",
    "#             webpage = requests.get(\"https://academic.naver.com/article.naver?doc_id=\" + str(href[22:31]))\r\n",
    "#             soup = BeautifulSoup(webpage.content, \"html.parser\")\r\n",
    "    \r\n",
    "#             try:\r\n",
    "#                 f.write(soup.find('h4',{\"id\":\"articleData\"}).text+'\\n') #제목\r\n",
    "#             except Exception as e:\r\n",
    "#                 continue\r\n",
    "\r\n",
    "\r\n",
    "f.close()"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-12-25327e8ad744>, line 178)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-12-25327e8ad744>\"\u001b[1;36m, line \u001b[1;32m178\u001b[0m\n\u001b[1;33m    f.close()\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "source": [
    "import requests\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "import json\r\n",
    "from collections import OrderedDict\r\n",
    "\r\n",
    "\r\n",
    "article_data = OrderedDict()\r\n",
    "id = 553446304\r\n",
    "webpage = requests.get(\"https://academic.naver.com/article.naver?doc_id=\" + str(id))\r\n",
    "soup = BeautifulSoup(webpage.content, \"html.parser\")\r\n",
    "\r\n",
    "# 초기 데이터 입력\r\n",
    "article_data[\"articleTitle\"] = soup.find('h4',{\"id\":\"articleData\"}).text\r\n",
    "article_data[\"articleId\"] = id\r\n",
    "article_data[\"link\"] = 'https://academic.naver.com/article.naver?doc_id='+ str(id)\r\n",
    "article_data[\"authorName\"] = soup.find('dd',{\"class\":\"ui_listdetail_item_info\"}).text\r\n",
    "article_data[\"citatedCount\"] = soup.select('div.type2 dt.ui_listdetail_item_tit+dd.ui_listdetail_item_info')[3].text.replace(\",\",\"\") # 형변환 불가. 확인필요\r\n",
    "article_data[\"category\"] = soup.find('dd',{\"class\":\"category_item\"}).text.replace('\\n',\"\").replace('\\t',\"\").replace('\\r',\"\")\r\n",
    "article_data[\"regDate\"] = int(soup.select('dd.ui_listdetail_item_info>span.ui_listdetail_box_txt+span.ui_listdetail_box_txt')[0].text[0:4])\r\n",
    "article_data[\"keyword\"] = soup.select('dd.category_item+dt.ui_listdetail_item_tit+dd.ui_listdetail_item_info')[0].text\r\n",
    "\r\n",
    "try:\r\n",
    "    article_data[\"abstract\"] = soup.find('p',{\"class\":\"ui_enddetail_txt\"}).text.replace(\"\\\"\",\"`\").strip()\r\n",
    "\r\n",
    "except Exception as e:\r\n",
    "    article_data[\"abstract\"] = \"\"\r\n",
    "    \r\n",
    "finally:\r\n",
    "    citation_data = OrderedDict()\r\n",
    "\r\n",
    "    # citation_data[\"aId\"] = 553446304\r\n",
    "    citation_data[\"aId\"] = article_data[\"articleId\"]\r\n",
    "    count = 0\r\n",
    "\r\n",
    "    abs_link=soup.select('ul.ui_listing_item a[href^=\"/article.naver?doc_id=\"]')\r\n",
    "\r\n",
    "    for i in soup.find_all(attrs={\"class\":\"ui_listing_subtit\"}) and abs_link:\r\n",
    "\r\n",
    "        href = i.attrs['href'] # a태그의 href 속성에서 추출\r\n",
    "    \r\n",
    "        # 참고문헌 table citation 컬럼 데이터를 json citation_data에 입력\r\n",
    "        cId = \"cId\"\r\n",
    "        cNumber = str(count+1)\r\n",
    "        cur_cId = cId+cNumber\r\n",
    "        citation_data[cur_cId] = int(href[22:31]) \r\n",
    "        count+=1\r\n",
    "\r\n",
    "# 데이터 저장\r\n",
    "\r\n",
    "with open('article_data','w',encoding=\"utf-8\") as make_file:\r\n",
    "    json.dump(article_data,make_file,ensure_ascii=False, indent=\"\\t\")\r\n",
    "\r\n",
    "with open('citation_data','w',encoding=\"utf-8\") as make_file:\r\n",
    "    json.dump(citation_data,make_file,ensure_ascii=False, indent=\"\\t\")\r\n",
    "\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "interpreter": {
   "hash": "908ecc93a7612be44523852fb7628e3056a00d88e370ca9a0402368ed5110b9a"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}